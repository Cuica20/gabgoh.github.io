{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b><h1> Stacked Approximation Regression Machines from First Principles </b></h1> (Gabriel Goh)</center>\n",
    "\n",
    "<img src = \"bases.png\">\n",
    "\n",
    "There has been some buzz on [reddit](https://www.reddit.com/r/MachineLearning/comments/50tbjp/stacked_approximated_regression_machine_a_simple/) about the paper [Stacked Approximated Regression Machine: A Simple Deep Learning Approach](https://arxiv.org/abs/1608.04062). Approaching the paper with a measured dose of skepticism, I was pleasantly surprised to find the paper containing a beautiful kernel of an idea - one I can see becoming a fixture in our deep learning toolkit, the $k$-Approximated Regression Machine ($k$-ARM). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background - Deep Nets\n",
    "\n",
    "A deep neural network is a function which takes as input, $x$, data, into an output, $F(x)$. The word “deep” in deep learning refers to the network being a composition of layer functions:\n",
    "$$F(x)=\\Psi^{m}(\\,\\Psi^{m-1}(\\,\\cdots\\,\\Psi^{1}(x)\\,\\cdots\\,)\\,)$$\n",
    "\n",
    "A traditional choice of a layer function looks like $\\Psi^{k}(x)=\\sigma(W^{T}_k x)$. Here $W^{T}_k$ is a matrix, representing a linear transform, (such as a convolution or a fully connected layer) and $\\sigma$ is a choice of a non-linear [activation function](https://en.wikipedia.org/wiki/Activation_function). The goal in deep learning is to shape our function $F$, by any means possible, by tweaking the weights till they fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sparse Regression Layer\n",
    "\n",
    "While traditional layers work well, there has been recently a Cambrian explosion of different kinds of layers. Consider the following conceptual layer. This layer isn't definied explicitly, but is instead defined implicitly as the solution of a *sparse coding problem*:\n",
    "$$\\Psi(x)=\\underset{y}{\\mbox{argmin }}\\big\\{\\tfrac{1}{2}\\|Wy-x\\|^{2}+\\lambda\\|y\\|_{1}\\big\\}.$$\n",
    "\n",
    "I dub this the sparse regression layer. \n",
    "\n",
    "This layer behaves very differently from a regular deep learning layer and it might be worthwhile to consider what it does. $W$ is a set of weights, to be trained, and $\\lambda$ is a parameter controlling how sparse the output is. Assuming a trained model, with sensible $W$, this layer is a sparse encoder. It takes $x$, it's input and transforms it into its representation in structural primitives (the columns of $W$). \n",
    "\n",
    "It is useful to think of a musical metaphor. The sound of a chord (our input $x$) is generated by a sparse superposition of sounds corrosponding to a small number of notes (our sparse parameter $y$). Our layer takes this generative process and throws it in reverse, and attempts to, from the sound of a chord, to recover the individual notes. \n",
    "\n",
    "If you have a background in engineering, these terms might be familiar to you in a different language. The generative process can be seen as the forward model, \n",
    "\n",
    "<img src=\"sparse.svg\" width = 450px>\n",
    "\n",
    "and the problem of recovering the notes from its output, the inverse problem\n",
    "\n",
    "<img src=\"inverse.svg\" width = 550px>\n",
    "\n",
    "This layer is therefore, paradoxically, the opposite of a regular deep learning layer. But it is this lopsided layer which makes the most sense, as it truly takes data from input space into explanation space - a higher level of abstraction. It's no surprise then that this is the mechanism proposed by [Olshausen et al](http://redwood.psych.cornell.edu/papers/olshausen_field_1997.pdf) to represent the early stages of visual processing.\n",
    "\n",
    "Despite its implicit definition, we can still take its (sub) gradient,\n",
    "$$\\begin{align}\\nabla\\Psi(x) & =W^{T}(Wy^\\star-x)+\\lambda\\mbox{sign}(y^\\star)\\\\\n",
    "\\frac{\\partial}{\\partial W_{ij}}\\Psi(x) & =\\frac{\\partial}{\\partial W_{ij}}\\frac{1}{2}\\|Wy^\\star-x\\|^{2}\n",
    "\\end{align}$$\n",
    "\n",
    "by plugging $y^\\star = \\Phi(x)$, the solution to the sparse coding problem. Hence there is no technical roadblock in integrating this into a deep learning framework. \n",
    "\n",
    "This leads to the troubling question - why isn't this kind of layer de-facto? The trouble with this layer is that it is expensive. Computing this layer requires the solution of a small optimization problem (on the order of the number of features in a datum). Measured in milliseconds, this may be cheap -  the cost of solving this problem is on the order of solving a linear system. But it is still an order of magnitude more expensive than a simple matrix multiplication passed through a nonlinearity. Adding a single layer like this would potentially increase the cost of every training step a hundredfold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $k$-ARM\n",
    "\n",
    "Let us instead try to approximate this layer. First, I define a (seemingly) cryptic operator\n",
    "$$\n",
    "\\Psi_{x}'(y) := \\sigma(y-\\alpha W^{T}(Wy-x)), \\qquad \\sigma(y)_{i}=\\mbox{sign}(y_i)\\max\\{0,\\left| y_{i} \\right|-\\lambda\\}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a number strictly greater than zero, but smaller than the largest eigenvector of $W^{T}W$. Let's think about what this operator does. Let $f(y)=\\tfrac{1}{2}\\|Wy-x\\|^{2}$, our quadratic term in the objective. The term inside the $\\sigma$ is then just $y - \\alpha\\nabla f(y)$, a gradient step. The nonlinearity, $\\sigma$, snaps all components of this new iterate between $[-\\lambda, \\lambda]$ to zero. So our operator consists of two steps. The first step improves the objective, and the second promotes sparsity. Furthermore, let's take these two facts as given:\n",
    "\n",
    "1. $\\Psi_x$ is the unique fixed point of the map $y\\mapsto \\Psi_{x}'(y),$ i.e. $\\Psi(x)=\\Psi_{x}'(\\Psi(x))$\n",
    "2. From any inital point $y$, repeated application of the map will always converge to $\\Psi(x).$\n",
    "\n",
    "In informal but suggestive notation, \n",
    "$$\\Psi(x)=\\Psi_{x}'(\\Psi_{x}'(\\Psi_{x}'(\\cdots))).$$\n",
    "\n",
    "Which suggests a series of approximations $\\Psi\\approx\\Psi_{k}$, (starting at $0$ for simplicity),\n",
    "$$\\begin{align}\n",
    "\\Psi_{0}(x) & =\\Psi_{x}'(\\mathbf{0})\\\\\n",
    "\\Psi_{1}(x) & =\\Psi_{x}'(\\Psi_{x}'(\\mathbf{0}))\\\\\n",
    "\\Psi_{2}(x) & =\\Psi'_{x}(\\Psi_{x}'(\\Psi_{x}'(\\mathbf{0})))\\\\\n",
    " & \\vdots\\\\\n",
    "\\lim_{k\\rightarrow\\infty}\\Psi_{k}(x) & =\\Psi(x).\n",
    "\\end{align}.$$\n",
    "\n",
    "This approximation has an architectural diagram looking like this\n",
    "\n",
    "<p>\n",
    "<img src = \"diagram.svg\" width = 500px>\n",
    "<p>\n",
    "\n",
    "And our most aggressive approximation, $\\Psi_0(x)$ has the form\n",
    "$$\n",
    "\\Psi_{0}(x)\t\n",
    "  =\\Psi_{x}'(\\mathbf{0})\n",
    "  =\\sigma(\\mathbf{0}-\\tfrac{1}{\\alpha}W^{T}(W\\mathbf{0}-x))\n",
    "  =\\sigma(\\tfrac{1}{\\alpha}W^{T}x)\n",
    "$$\n",
    "\n",
    "which should look familiar. Why, it's nothing more than our traditional layer discussed at the beginning! A tantalizing coincidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Generalized $k$-ARM\n",
    "\n",
    "##### Proximal Gradient\n",
    "Let's take a step back and think about the operator $\\Psi_{x}'(y)$. Where does it come from? There is, in fact, a powerful framework in convex optimization which gives us the tools to craft such operators. The study of proximal operators gives us a means to solve problems of the form\n",
    "$$y^\\star = \\mbox{argmin}\\{f(y)+g(y)\\}$$\n",
    "\n",
    "Where $f$ is smooth and convex and $g$ is just convex. For reasons which will be clear later, $f$ is usually chosen to be the vessel of our data, and $g$ to be structurally simple, like the one norm. Take sparse coding, discussed earlier, a special case of this where\n",
    "$$f(y)=\\tfrac{1}{2}\\|Wy-x\\|^{2},\\qquad g(y)=\\lambda\\|y\\|_{1}$$\n",
    "\n",
    "This framework gives us a recipie on how to replace the one norm with any convex function $g$, such as the 2-norm. \n",
    "##### The Proximal Gradient Iteration\n",
    "\n",
    "Given $f$ and $g$, the proximal gradient method defines the map\n",
    "$$\\Psi_{f}^{'}[g](x)=\\sigma[g](y_{k}+\\alpha\\nabla f(y)),\\qquad\\sigma[g](y)=\\mbox{argmin}\\{\\tfrac{1}{2}\\|\\bar{y}-y\\|^{2}+g(y)\\}$$\n",
    "\n",
    "so that, you guessed it\n",
    "\n",
    "1. $y^\\star$ is the unique fixed point of the map $y\\mapsto \\Psi_{f}'[g](y)$, i.e. $y^\\star = \\Psi_{f}'[g](y^\\star)$\n",
    "2. From any $y$, for small enough $\\alpha>0$ repeated application of the map will always converge to $y^\\star$\n",
    "\n",
    "See Boyd's [slides](https://people.eecs.berkeley.edu/~elghaoui/Teaching/EE227A/lecture18.pdf) for more details. A few notes are in order. First, though you can in principle stick in any $g$ you want, you cannot escape from having to compute $\\sigma_g$, the proximal term. This method hinges on when $\\sigma_g$ having something resembling a closed form solution. \n",
    "\n",
    "Second, this framework encompasses constrained optimization as\n",
    "$$\\underset{y\\in S}{\\mbox{argmin}}\\{f(y)\\}=\\underset{y}{\\mbox{argmin}}\\{f(y)+\\delta(y \\, | \\, S)\\},\\qquad\\delta(y)=\\begin{cases}\n",
    "0 & y\\in S\\\\\n",
    "\\infty & y\\notin S\n",
    "\\end{cases}.$$\n",
    "\n",
    "So the constrained problem can be simulated by using $g(y) = \\delta(y \\, | \\, S)$.\n",
    "\n",
    "Examples of this are \n",
    "$$\\begin{alignat*}{2}\n",
    " & g(y)=0 &  & \\sigma[g](y)=y\\\\\n",
    " & g(y)=\\lambda\\|y\\|_{1} &  & \\sigma[g](y)_{i}=\\mbox{sign}(x_{i})(\\left|x_{i}\\right|-\\lambda)_{i}\\\\\n",
    " & g(y)=\\lambda\\|y\\|_{2} &  & \\sigma[g](y)=\\max\\{0,1-\\lambda/\\|y\\|_{2}\\}y\\\\\n",
    " & g(y)=\\delta(y,\\mathbf{R}_{+}) & \\qquad & \\sigma[g](y)_{i}=\\max\\{0,y_{i}\\}\\\\\n",
    " & g(y)=\\delta(y,\\mathbf{R}_{+})+\\lambda\\|y\\|_{1} & \\qquad & \\sigma[g](y)_{i}=\\max\\{0,y_{i}-\\lambda\\}\n",
    "\\end{alignat*}$$\n",
    "\n",
    "##### A new cornucopia of ARMs\n",
    "\n",
    "With a choice of $g$, now have the freedom to construct a whole spectrum of Regression Machines, \n",
    "$$\\Psi[g](x)=\\underset{y}{\\mbox{argmin }}\\big\\{\\tfrac{1}{2}\\|Wy-x\\|^{2}+g(y)\\big\\}$$\n",
    "With corrosponding ARMs defined using the map\n",
    "$$\\Psi[g]_{x}'(y):=\\sigma[g](y-\\alpha W^{T}(Wy-x))$$\n",
    "\n",
    "Take for example the Non-Negative Regression Machine, with $g(x)=\\delta(x,{\\mathbf R}_+)$. This has\n",
    "$$\\Psi[\\delta(\\cdot|\\mathbf{R}_{+})]=\\underset{y\\geq0}{\\mbox{argmin }}\\tfrac{1}{2}\\|Wy-x\\|^{2},\\quad\\Psi[\\delta(\\cdot|\\mathbf{R}_{+})]_{x}'(y):=\\max\\{0,y-\\alpha W^{T}(Wy-x)\\}$$\n",
    "With a 0-ARM of\n",
    "$$\\Psi[\\delta(\\cdot|\\mathbf{R}_{+})]_{0}(y)=\\max\\{0,W^{T}y\\},$$\n",
    "\n",
    "exactly a traditional layer with `ReLu` Activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on training the Network\n",
    "\n",
    "Finding $W$'s for the network still remains a nagging problem. The usual set of tools apply, and given a large labeled dataset, a simple application of backpropagation on the $k$-ARM still works the way it should.\n",
    "\n",
    "The paper proposes a heurstic for initalizing $W$'s, however, based on Dictionary Learning. Assuming a dataset {$x_1,\\dots,x_n$} which posesses the sparse linear structure described in the beginning, one could try to learn both the $y_i$'s and $W$'s simultaniously and dictionary simultaneously in the joint optimization problem\n",
    "$$\\underset{W,y_{1},\\dots,y_{n}}{\\mbox{argmin}}\\left\\{ \\sum_{i=1}^{n}\\left(\\tfrac{1}{2}\\|Wy_{i}-x_{i}\\|^{2}+\\lambda\\|y_{i}\\|_{1}\\right)\\right\\}.$$\n",
    "\n",
    "Note the similarity of this to PCA. In PCA, you are too trying to find an explanation of the data in terms of linear combinations of basis vectors, but there is no restriction of sparsity. The method of choice in the paper for solving this problem is the $k$-PCA.\n",
    "\n",
    "Thus, $k$-PCA becomes the basis for the heuristic of traing stacked layers. It works as follows\n",
    "\n",
    "- Take the raw data, $x_i$, and solve the Dictionary Learning problem to yield $W$ and $y_i$'s.\n",
    "- Use $y_i$'s as the raw data for the next layer, and repeat.\n",
    "\n",
    "This technique eerily recalls our forgotten hero the [Restricted Boltzman Machine (RBM)](https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine), which kick-started the rebirth of the neural net. It was a clever idea which worked, but had one drawback - nobody quite understood *how* it worked. Unsatisfied with this mystery, the deep learning community seemed happy to ditch it when it learned how to train networks with gradient descent alone. The interpretation of a deep neural net as doing approximate of sparse coding, however, gives us a reason to revisit the method of unsupervised pre-training.\n",
    "\n",
    "Training the stacked ARM still remains heuristic, but at least we can rest easy knowing it is *less* of a heuristic. And the most valuable proof is in the pudding. The stacked ARMs show exciting results on training data, and is a great first step in taking the deep, dark mysteries of deep learning and putting them where they should be - in a footnote of history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Oh, and here's a link to my [website](http://gabgoh.github.io).\n",
    "\n",
    "Picture in the title, as well as the sparse encoding picture, is taken from [Olshausen et al, Sparse Coding With An Overcomplete Basis Set: A Strategy Employed by V12?](http://redwood.psych.cornell.edu/papers/olshausen_field_1997.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "/*div#maintoolbar, div#header {display: none !important;}\n",
       "*/\n",
       "\n",
       "#notebook {\n",
       "  padding-bottom: 20px\n",
       "}\n",
       "\n",
       "#notebook-container {\n",
       "  font-family: Computer Modern\",Helvetica,Arial,sans-serif;\n",
       "  font-size:14px !important;\n",
       "  width:800px;\n",
       "  padding: 20px;\n",
       "  background-color: #FFF;\n",
       "  min-height: 10;\n",
       "  border: 0px solid;\n",
       "  box-shadow: 0px 0px 0px 0px !important;\n",
       "}\n",
       "\n",
       "\n",
       "div#notebook{\n",
       "font-size:16px;\n",
       "    line-height: 1.42857143;\n",
       "\n",
       "}\n",
       "\n",
       "body {\n",
       "  background-color: white !important;\n",
       "}\n",
       "\n",
       ".MathJax_Display: {\n",
       "  margin: 20px\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: Computer Modern; src: url(\"http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf\");\n",
       "}\n",
       "\n",
       "\n",
       "div.cell{\n",
       "  width:700px;\n",
       "  margin-left:auto;\n",
       "  margin-right:auto;\n",
       "}\n",
       "\n",
       "div.output_subarea {\n",
       "    overflow-x: auto;\n",
       "    padding: 0.4em;\n",
       "    -webkit-box-flex: 1;\n",
       "    -moz-box-flex: 1;\n",
       "    box-flex: 1;\n",
       "    flex: 1;\n",
       "    max-width: calc(100%);\n",
       "}\n",
       "\n",
       ".output_wrapper {\n",
       "  padding-top : 10px;\n",
       "}\n",
       "\n",
       "div.input_area {\n",
       "    border: 0px solid #cfcfcf;\n",
       "    border-radius: 2px;\n",
       "    background: rgba(255, 255, 255, 1);\n",
       "    line-height: 1.21429em;\n",
       "    padding: 10px;\n",
       "}\n",
       "\n",
       "h1 { font-family: \"Helvetica Neue\",Helvetica,Arial,sans-serif; }\n",
       "\n",
       ".prompt{ display:None; }\n",
       "\n",
       ".output_png img {\n",
       "    display: block !important;\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".output_svg > div{\n",
       "margin-left:auto !important;\n",
       "margin-right:auto !important;\n",
       "}\n",
       "\n",
       ".ui-wrapper {\n",
       "  margin-left:auto !important;\n",
       "  margin-right:auto !important\n",
       "}\n",
       "\n",
       "hr {\n",
       "    display: block;\n",
       "    height: 1px;\n",
       "    border: 0;\n",
       "    border-top: 1px solid #000;\n",
       "    margin: 1em 0;\n",
       "    padding: 0;\n",
       "}\n",
       "\n",
       ".err{\n",
       "  border : 0px !important;\n",
       "}\n",
       "\n",
       ".MathJax_Display{\n",
       "  padding-top: 12px;\n",
       "}\n",
       "\n",
       "\n",
       ".hl-ipython3 {\n",
       "  display: none\n",
       "}\n",
       "\n",
       "</style>\n",
       "\n",
       "<script>\n",
       "  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n",
       "  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n",
       "  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n",
       "  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n",
       "\n",
       "  ga('create', 'UA-65931696-1', 'auto');\n",
       "  ga('send', 'pageview');\n",
       "\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(open(\"style.css\",\"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

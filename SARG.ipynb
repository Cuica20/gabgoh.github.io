{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b><h1> Stacked Approximation Regression Machines from First Principles </b></h1></center>\n",
    "\n",
    "<img src = \"bases.png\">\n",
    "\n",
    "There has been some buzz on [reddit](https://www.reddit.com/r/MachineLearning/comments/50tbjp/stacked_approximated_regression_machine_a_simple/) about the paper [Stacked Approximated Regression Machine: A Simple Deep Learning Approach](https://arxiv.org/abs/1608.04062). Approaching the paper with a measured dose of skepticism, I was pleasantly surprised to find the paper containing a beautiful kernel of an idea - one I can see becoming a fixture in our deep learning toolkit. This is a new kind of layer known as the $k$-Approximated Regression Machine ($k$-ARM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Background - Deep Nets\n",
    "\n",
    "A deep neural network is a function which takes an input $x$ into an output, $F(x)$. The word “deep” in deep learning refers to the network being a composition of layer functions, like so\n",
    "$$F(x)=\\Psi^{1}(\\,\\Psi^{2}(\\, \\cdots \\,\\Psi^{k}(x)\\,\\cdots\\,) \\,).$$\n",
    "\n",
    "A traditional choice of a layer function looks like $\\Psi^{k}(x)=\\sigma(W^{T}_k x)$. Here $W^{T}_k$ is a matrix, representing a linear transform, such as a convolution or a fully connected layer. $\\sigma$ is a choice of a non-linear [activation function](https://en.wikipedia.org/wiki/Activation_function). The goal in deep learning is to shape our function $F$, by any means possible, by tweaking the weights till they fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Sparse Regression Layer\n",
    "\n",
    "Moving away from traditional layers, here is hypothetical one. This layer, $\\Psi$ does not admit a formula, but is defined instead implicitly as the solution of a sparse coding problem\n",
    "$$\\Psi(x)=\\underset{y}{\\mbox{argmin }}\\big\\{\\tfrac{1}{2}\\|Wy-x\\|^{2}+\\lambda\\|y\\|_{1}\\big\\}.$$\n",
    "I dub this the sparse regression layer. This layer maps its inputs $x$ to the weights of the best sparse linear combinations of $W$ which best reconstruct $x$. \n",
    "\n",
    "<img src=\"sparse.svg\" width = 450px>\n",
    "\n",
    "Despite its implicit definition, we can still take it's (sub) gradient,\n",
    "$$\\begin{align}\n",
    "\\nabla\\Psi(x) & =W^{T}(Wy^{\\star}-x)+\\lambda\\mbox{sign}(y^{\\star})\\\\\n",
    "\\frac{\\partial}{\\partial W}\\Psi(x) & =\\frac{\\partial}{\\partial W}\\frac{1}{2}\\|Wy^{\\star}-x\\|^{2}\n",
    "\\end{align}$$\n",
    "\n",
    "where $y^{\\star} =\\Psi(x)$ and hence integrate it into any modular deep learning framework. Unfortunately computing this layer requires the solution of a small optimization problem (on the order of the number of features in a datapoint). This may be cheap, but it is still an order of magnitude more expensive than a simple matrix multiplication passed through a nonlinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The $k$ Approximate Sparse Regression Layer\n",
    "\n",
    "Let us instead try to approximate this function. First, I will define a cryptic operator\n",
    "$$\n",
    "\\Psi_{x}'(y) := \\sigma(y-\\tfrac{\\alpha}{2} W^{T}(Wy-x)), \\qquad \\sigma(x)_{i}=\\mbox{sign}(x_{i})(\\left| x_{i} \\right|-\\lambda)_{i}\n",
    "$$\n",
    "\n",
    "and $\\alpha$ is a small number which depends on the eigenvectors of $W^{T}W$. Let's take the following two facts as given\n",
    "\n",
    "1. $\\Psi_x$ is the unique fixed point of the map $y\\mapsto \\Psi_{x}'(y),$ i.e. $\\Psi(x)=\\Psi_{x}'(\\Psi(x))$\n",
    "2. From any inital point $y$, repeated application of the map will always converge to $\\Psi(x).$\n",
    "\n",
    "In informal but suggestive notation, \n",
    "$$\\Psi(x)=\\Psi_{x}'(\\Psi_{x}'(\\Psi_{x}'(\\cdots))).$$\n",
    "\n",
    "Which suggests this series of approximations $\\Psi\\approx\\Psi_{k}$, starting at $0$ for simplicity,\n",
    "$$\\begin{align}\n",
    "\\Psi_{0}(x) & =\\Psi_{x}'(\\mathbf{0})\\\\\n",
    "\\Psi_{1}(x) & =\\Psi_{x}'(\\Psi_{x}'(\\mathbf{0}))\\\\\n",
    "\\Psi_{2}(x) & =\\Psi'_{x}(\\Psi_{x}'(\\Psi_{x}'(\\mathbf{0})))\\\\\n",
    " & \\vdots\\\\\n",
    "\\lim_{k\\rightarrow\\infty}\\Psi_{k}(x) & =\\Psi(x).\n",
    "\\end{align}.$$\n",
    "\n",
    "With an architectural diagram looking like this\n",
    "\n",
    "<p>\n",
    "<img src = \"diagram.svg\" width = 500px>\n",
    "<p>\n",
    "\n",
    "And our most aggressive approximation, $\\Psi_0(x)$ has the form\n",
    "$$\n",
    "\\Psi_{0}(x)\t\n",
    "  =\\Psi_{x}'(\\mathbf{0})\n",
    "  =\\sigma(\\mathbf{0}-\\tfrac{1}{\\alpha}W^{T}(W\\mathbf{0}-x))\n",
    "  =\\sigma(\\tfrac{1}{\\alpha}W^{T}x)\n",
    "$$\n",
    "\n",
    "which should look familiar. Why, it's nothing more than our generic layer discussed at the beginning! Now, bear in mind this be nothing more than a tantalizing coincidence. But read on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Proximal Gradient Descent\n",
    "\n",
    "But hold on, you say. Where did $\\Psi_x'$ come from? IF you're familiar at all with $L_1$ regularization, this may be all too familiar to you. Indeed, the map  $\\Psi_x'$ comes from a general framework which solves problems with the structure\n",
    "$$y^{\\star}=\\mbox{argmin}\\{f(y)+g(y)\\}$$\n",
    "\n",
    "Where $f$ is smooth + convex and $g$ is just convex. The sparse coding problem uses this structure, with\n",
    "$$f(y)=\\tfrac{1}{2}\\|Wy-x\\|^{2},\\qquad g(y)=\\lambda\\|y\\|_{1}$$\n",
    "\n",
    "But the generality of this framework allows you to replace $g$ with a whole variety of other functions, including constrained optimization, with a bit of syntactic sugar:\n",
    "$$y^{\\star}=\\underset{y}{\\mbox{argmin}}\\{f(y)+\\delta(y \\, | \\, S)\\}=\\underset{y\\in S}{\\mbox{argmin}}\\{f(y)\\},\\qquad\\delta(y)=\\begin{cases}\n",
    "0 & y\\in S\\\\\n",
    "\\infty & y\\notin S\n",
    "\\end{cases}.$$\n",
    "Given $f$ and $g$, the proximal gradient method defines the map\n",
    "$$\\Psi_{f}^{'}(x)=\\sigma_{g}(y_{k}+\\alpha\\nabla f(y)),\\qquad\\sigma_{g}(y)=\\mbox{argmin}\\{\\tfrac{1}{2}\\|\\bar{y}-y\\|^{2}+g(y)\\}$$\n",
    "\n",
    "so that, you guessed it\n",
    "\n",
    "1. $y^\\star$ is the unique fixed point of the map $y\\mapsto \\Psi_{f}'(y)$, i.e. $y^\\star = \\Psi_{f}'(y^\\star)$\n",
    "2. From any $y$, for small enough $\\alpha>0$ repeated application of the map will always converge to $y^\\star$\n",
    "\n",
    "The details of the theory have been covered in detail in the Boyd's [slides](https://people.eecs.berkeley.edu/~elghaoui/Teaching/EE227A/lecture18.pdf) but here is just a sampling of different proximal functions for differents $g$'s\n",
    "$$\\begin{alignat*}{2}\n",
    " & g(y)=0 &  & \\sigma_{g}(y)=y\\\\\n",
    " & g(y)=\\lambda\\|y\\|_{1} &  & \\sigma_{g}(y)_{i}=\\mbox{sign}(x_{i})(\\left|x_{i}\\right|-\\lambda)_{i}\\\\\n",
    " & g(y)=\\lambda\\|y\\|_{2} &  & \\sigma_{g}(y)=\\max\\{0,1-\\lambda/\\|y\\|_{2}\\}y\\\\\n",
    " & g(y)=\\delta(y,\\mathbf{R}_{+}) & \\qquad & \\sigma_{g}(y)_{i}=\\max\\{0,y_{i}\\}\\\\\n",
    " & g(y)=\\delta(y,\\mathbf{R}_{+})+\\lambda\\|y\\|_{1} & \\qquad & \\sigma_{g}(y)_{i}=\\max\\{0,y_{i}-\\lambda\\}\\\\\n",
    " & g(y)=\\delta(y,\\{\\bar{y}\\mid\\bar{y}_{i}\\in[0,\\alpha]\\}) &  & \\sigma_{g}(y)_{i}=\\max\\{y_{i},\\alpha y_{i}\\}\n",
    "\\end{alignat*}$$\n",
    "\n",
    "Which again, look tantalizingly like activation functions, including the `ReLu` and the `Leaky ReLu`. Time will bear this all out, but for now the SARMs show impressive benchmarks with a minimal amount of training, and this idea is a worthy one for meditation.\n",
    "\n",
    "Oh, and here's a link to my [website](http://gabgoh.github.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "/*div#maintoolbar, div#header {display: none !important;}\n",
       "*/\n",
       "\n",
       "#notebook {\n",
       "  padding-bottom: 20px\n",
       "}\n",
       "\n",
       "#notebook-container {\n",
       "  font-family: Computer Modern\",Helvetica,Arial,sans-serif;\n",
       "  font-size:14px !important;\n",
       "  width:800px;\n",
       "  padding: 20px;\n",
       "  background-color: #FFF;\n",
       "  min-height: 10;\n",
       "  border: 0px solid;\n",
       "  box-shadow: 0px 0px 0px 0px !important;\n",
       "}\n",
       "\n",
       "\n",
       "div#notebook{\n",
       "font-size:16px;\n",
       "    line-height: 1.42857143;\n",
       "\n",
       "}\n",
       "\n",
       "body {\n",
       "  background-color: white !important;\n",
       "}\n",
       "\n",
       ".MathJax_Display: {\n",
       "  margin: 20px\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: Computer Modern; src: url(\"http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf\");\n",
       "}\n",
       "\n",
       "\n",
       "div.cell{\n",
       "  width:700px;\n",
       "  margin-left:auto;\n",
       "  margin-right:auto;\n",
       "}\n",
       "\n",
       "div.output_subarea {\n",
       "    overflow-x: auto;\n",
       "    padding: 0.4em;\n",
       "    -webkit-box-flex: 1;\n",
       "    -moz-box-flex: 1;\n",
       "    box-flex: 1;\n",
       "    flex: 1;\n",
       "    max-width: calc(100%);\n",
       "}\n",
       "\n",
       ".output_wrapper {\n",
       "  padding-top : 10px;\n",
       "}\n",
       "\n",
       "div.input_area {\n",
       "    border: 0px solid #cfcfcf;\n",
       "    border-radius: 2px;\n",
       "    background: rgba(255, 255, 255, 1);\n",
       "    line-height: 1.21429em;\n",
       "    padding: 10px;\n",
       "}\n",
       "\n",
       "h1 { font-family: \"Helvetica Neue\",Helvetica,Arial,sans-serif; }\n",
       "\n",
       ".prompt{ display:None; }\n",
       "\n",
       ".output_png img {\n",
       "    display: block !important;\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".output_svg > div{\n",
       "margin-left:auto !important;\n",
       "margin-right:auto !important;\n",
       "}\n",
       "\n",
       ".ui-wrapper {\n",
       "  margin-left:auto !important;\n",
       "  margin-right:auto !important\n",
       "}\n",
       "\n",
       "hr {\n",
       "    display: block;\n",
       "    height: 1px;\n",
       "    border: 0;\n",
       "    border-top: 1px solid #000;\n",
       "    margin: 1em 0;\n",
       "    padding: 0;\n",
       "}\n",
       "\n",
       ".err{\n",
       "  border : 0px !important;\n",
       "}\n",
       "\n",
       ".MathJax_Display{\n",
       "  padding-top: 12px;\n",
       "}\n",
       "\n",
       "\n",
       "</style>\n",
       "\n",
       "<script>\n",
       "  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n",
       "  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n",
       "  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n",
       "  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n",
       "\n",
       "  ga('create', 'UA-65931696-1', 'auto');\n",
       "  ga('send', 'pageview');\n",
       "\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(open(\"style.css\",\"r\").read())\n",
    "# Yes I wrote this in ipython."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

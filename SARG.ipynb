{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b><h1> Stacked Approximation Regression Machines from First Principles </b></h1></center>\n",
    "\n",
    "There has been some buzz on [reddit](https://www.reddit.com/r/MachineLearning/comments/50tbjp/stacked_approximated_regression_machine_a_simple/) about the paper [Stacked Approximated Regression Machine: A Simple Deep Learning Approach](https://arxiv.org/abs/1608.04062). Approaching the paper with a measured dose of skeptcism, I was pleasantly surprised to find the paper containing a beautiful kernel of an idea - one I can see quickly becoming a fixture in our deep learning toolkit. This is a new kind of layer known as the $k$-Approximated Regression Machine ($k$-ARM). Here is my attempt to explain this layer from first principles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Background\n",
    "\n",
    "To establish notation, recall a deep neural network is a function which takes an input $x$ into an output, $F(x)$. The word “deep” in deep learning refers to the network being a composition of layer functions,  \n",
    "$$F(x)=\\Psi^{1}(\\Psi^{2}( \\cdots \\,\\Psi^{k}(x)\\,\\cdots) )$$\n",
    "\n",
    "The most traditional choice of a layer function takes the form $\\Psi^{k}(x)=\\sigma(W^{T}_k x)$. Here $W^{T}_k$ is a matrix, representing a linear transform such as a convolution, and $\\sigma$ is a choice of a non-linear [activation function](https://en.wikipedia.org/wiki/Activation_function). The goal in deep learning is to shape our function $F$, by any means possible, by tweaking the weights till they fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Sparse Regression Layer\n",
    "\n",
    "As we move away from traditional layers, we enter a facinating zoo of possible tinker toys which we can use in our model. Here I will define one such hypothetical layer, defined implicitly as the solution of a sparse coding problem\n",
    "\n",
    "$$\\Psi(x)=\\underset{y}{\\mbox{argmin }}\\big\\{\\tfrac{1}{2}\\|Wy-x\\|^{2}+\\lambda\\|y\\|_{1}\\big\\}.$$\n",
    "\n",
    "I dub this the sparse regression layer, the layer maps its inputs $x$ to the weights of the best sparse linear combinations of $W$ which best reconstruct $x$. \n",
    "\n",
    "<img src=\"sparse.svg\" width = 450px>\n",
    "\n",
    "Despite its implicit definition, we can still take it's (sub) gradient,\n",
    "$$\\begin{align}\n",
    "\\nabla\\Psi(x) & =W^{T}(Wy^{\\star}-x)+\\lambda\\mbox{sign}(y^{\\star})\\\\\n",
    "\\frac{\\partial}{\\partial W}\\Psi(x) & =\\frac{\\partial}{\\partial W}\\frac{1}{2}\\|Wy^{\\star}-x\\|^{2}\n",
    "\\end{align}$$\n",
    "\n",
    "where $y^{\\star} =\\Psi(x)$ and hence integrate it into any modular deep learning framework. Unfortunately this is kinda expensive. Not ridiculously expensive, this is a small optimization problem (on the order of the number of features in a datapoint), but still an order of magnitude more difficult than a simple matrix multiplication passed through a nonlinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The $k$ Approximate Sparse Regression Layer\n",
    "\n",
    "Let us, instead try to approximate this function. It is a well known fact that $\\Psi(x)$ is a fixed point of the map \n",
    "$$y\\mapsto\\Psi_{x}'(y)$$\n",
    "\n",
    "Where\n",
    "$$\n",
    "\\Psi_{x}'(y) :=\\sigma(y-\\tfrac{\\alpha}{2}\\nabla\\|W\\cdot-x\\|^{2}(y))\n",
    " =\\sigma(y-\\tfrac{\\alpha}{2} W^{T}(Wy-x))\n",
    "$$\n",
    "\n",
    "and $\\sigma$ is the soft threshold function\n",
    "$$\\sigma(x)_{i}=\\mbox{sign}(x_{i})(\\left| x_{i} \\right|-\\lambda)_{i},$$\n",
    "\n",
    "and $\\alpha$ is a small number which depends on the eigenvalues of $L^T L$. If you're familiar at all with $L_1$ regularization, this iteration will be familiar to you as by one of it's many aliases, Iterative Soft Thresholding (or by proxy of his famous cousion, FISTA), [Proximal Gradient](http://www.seas.ucla.edu/~vandenbe/236C/lectures/proxgrad.pdf) or by it's oldest name, Forward-Backwards Splitting. In informal but suggestive notation, \n",
    "$$\\Psi(x)=\\Psi_{x}'(\\Psi_{x}'(\\Psi_{x}'(\\cdots)))$$\n",
    "\n",
    "The above formula suggest this series of approximations $\\Psi\\approx\\Psi_{k}$ where\n",
    "$$\\begin{align}\n",
    "\\Psi_{0}(x) & =\\Psi_{x}'(\\mathbf{0})\\\\\n",
    "\\Psi_{1}(x) & =\\Psi_{x}'(\\Psi_{x}'(\\mathbf{0}))\\\\\n",
    "\\Psi_{2}(x) & =\\Psi'_{x}(\\Psi_{x}'(\\Psi_{x}'(\\mathbf{0})))\\\\\n",
    " & \\vdots\\\\\n",
    "\\lim_{k\\rightarrow\\infty}\\Psi_{k}(x) & =\\Psi(x).\n",
    "\\end{align}$$\n",
    "\n",
    "We take the initial point as ${\\bf 0}$ for simplicity. Let's take a look at $\\Psi_{0}$. \n",
    "$$\n",
    "\\Psi_{0}(x)\t\n",
    "  =\\Psi_{x}'(\\mathbf{0})\n",
    "  =\\sigma(\\mathbf{0}-\\tfrac{1}{\\alpha}W^{T}(W\\mathbf{0}-x))\n",
    "  =\\sigma(\\tfrac{1}{\\alpha}W^{T}x)\n",
    "$$\n",
    "\n",
    "Aha! This should look familiar! Why, it's nothing more than our generic layer discussed at the beginning! So I will do my best impression of Geoff Hinton and say, “$\\Psi$ can be seen as a deep neural network with tied weights and an infinite number of layers.\" to which I add the footnote - to which we approximate with a $k$-ARM. Architecturally, the $k$-ARM looks like\n",
    "\n",
    "<p>\n",
    "<img src = \"diagram.svg\" width = 500px>\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fascinating insight here is in what deep nets may have been secretly doing regression all along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "/*div#maintoolbar, div#header {display: none !important;}\n",
       "*/\n",
       "\n",
       "#notebook {\n",
       "  padding-bottom: 20px\n",
       "}\n",
       "\n",
       "#notebook-container {\n",
       "  font-family: Computer Modern\",Helvetica,Arial,sans-serif;\n",
       "  font-size:14px !important;\n",
       "  width:800px;\n",
       "  padding: 20px;\n",
       "  background-color: #FFF;\n",
       "  min-height: 10;\n",
       "  border: 0px solid;\n",
       "  box-shadow: 0px 0px 0px 0px !important;\n",
       "}\n",
       "\n",
       "\n",
       "div#notebook{\n",
       "font-size:16px;\n",
       "    line-height: 1.42857143;\n",
       "\n",
       "}\n",
       "\n",
       "body {\n",
       "  background-color: white !important;\n",
       "}\n",
       "\n",
       ".MathJax_Display: {\n",
       "  margin: 20px\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: Computer Modern; src: url(\"http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf\");\n",
       "}\n",
       "\n",
       "\n",
       "div.cell{\n",
       "  width:700px;\n",
       "  margin-left:auto;\n",
       "  margin-right:auto;\n",
       "}\n",
       "\n",
       "div.output_subarea {\n",
       "    overflow-x: auto;\n",
       "    padding: 0.4em;\n",
       "    -webkit-box-flex: 1;\n",
       "    -moz-box-flex: 1;\n",
       "    box-flex: 1;\n",
       "    flex: 1;\n",
       "    max-width: calc(100%);\n",
       "}\n",
       "\n",
       ".output_wrapper {\n",
       "  padding-top : 10px;\n",
       "}\n",
       "\n",
       "div.input_area {\n",
       "    border: 0px solid #cfcfcf;\n",
       "    border-radius: 2px;\n",
       "    background: rgba(255, 255, 255, 1);\n",
       "    line-height: 1.21429em;\n",
       "    padding: 10px;\n",
       "}\n",
       "\n",
       "h1 { font-family: \"Helvetica Neue\",Helvetica,Arial,sans-serif; }\n",
       "\n",
       ".prompt{ display:None; }\n",
       "\n",
       ".output_png img {\n",
       "    display: block !important;\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".output_svg > div{\n",
       "margin-left:auto !important;\n",
       "margin-right:auto !important;\n",
       "}\n",
       "\n",
       ".ui-wrapper {\n",
       "  margin-left:auto !important;\n",
       "  margin-right:auto !important\n",
       "}\n",
       "\n",
       "hr {\n",
       "    display: block;\n",
       "    height: 1px;\n",
       "    border: 0;\n",
       "    border-top: 1px solid #000;\n",
       "    margin: 1em 0;\n",
       "    padding: 0;\n",
       "}\n",
       "\n",
       ".err{\n",
       "  border : 0px !important;\n",
       "}\n",
       "\n",
       ".MathJax_Display{\n",
       "  padding-top: 12px;\n",
       "}\n",
       "\n",
       "\n",
       "</style>\n",
       "\n",
       "<script>\n",
       "  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n",
       "  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n",
       "  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n",
       "  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n",
       "\n",
       "  ga('create', 'UA-65931696-1', 'auto');\n",
       "  ga('send', 'pageview');\n",
       "\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(open(\"style.css\",\"r\").read())\n",
    "# Yes I wrote this in ipython."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
